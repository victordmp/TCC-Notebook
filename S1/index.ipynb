{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /Users/victorpires/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /Users/victorpires/Library/Python/3.9/lib/python/site-packages (2.2.2)\n",
      "Requirement already satisfied: sklearn in /Users/victorpires/Library/Python/3.9/lib/python/site-packages (0.0)\n",
      "Collecting arff\n",
      "  Downloading arff-0.9.tar.gz (4.7 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/victorpires/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/victorpires/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/victorpires/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/victorpires/Library/Python/3.9/lib/python/site-packages (from sklearn) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/victorpires/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/victorpires/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/victorpires/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (1.4.0)\n",
      "Building wheels for collected packages: arff\n",
      "  Building wheel for arff (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for arff: filename=arff-0.9-py3-none-any.whl size=4970 sha256=1e6bd084063a953659777e06548414da476585f0da89653094cabd454812e26c\n",
      "  Stored in directory: /Users/victorpires/Library/Caches/pip/wheels/5e/d6/66/a25682c020fb563800fea1a06de58e4684243efd68cb83db83\n",
      "Successfully built arff\n",
      "Installing collected packages: arff\n",
      "Successfully installed arff-0.9\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy pandas sklearn arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import arff\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weka_tokenizer(doc):\n",
    "    # Expressão regular usada na tokenização \n",
    "    delimiters_regexp = re.compile(\"[ |\\n|\\f|\\r|\\t|.|,|;|:|'|\\\"|(|)|?|!]\")\n",
    "    return list(filter(None, delimiters_regexp.split(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initDataset(fileName, vocabularyFile):\n",
    "    fileExtension = fileName.split('.')\n",
    "    if (fileExtension[1] == 'csv'):\n",
    "        df = pd.read_csv(fileName)\n",
    "    else:\n",
    "        data = arff.load(fileName)\n",
    "        df = pd.DataFrame(data, columns=['token', 'loc', 'abstract_keyword', 'assert_keyword', 'boolean_keyword', 'break_keyword', 'byte_keyword', 'case_keyword', 'catch_keyword', 'char_keyword', 'class_keyword', 'continue_keyword', 'default_keyword', 'do_keyword', 'double_keyword', 'else_keyword', 'enum_keyword', 'exports_keyword', 'extends_keyword', 'final_keyword', 'finally_keyword', 'float_keyword', 'for_keyword', 'if_keyword', 'implements_keyword', 'import_keyword', 'instanceof_keyword', 'int_keyword', 'interface_keyword', 'long_keyword', 'modules_keyword', 'native_keyword', 'new_keyword', 'package_keyword', 'private_keyword', 'protected_keyword', 'public_keyword', 'requires_keyword', 'return_keyword', 'short_keyword', 'static_keyword', 'strictfp_keyword', 'super_keyword', 'switch_keyword', 'synchronized_keyword', 'this_keyword', 'throw_keyword', 'throws_keyword', 'transient_keyword', 'try_keyword', 'void_keyword', 'volatile_keyword', 'while_keyword', 'true_keyword', 'null_keyword', 'false_keyword', 'const_keyword', 'goto_keyword', 'keywordcount', 'klass'])\n",
    "\n",
    "    y = df['klass']\n",
    "\n",
    "    if (fileExtension[1] == 'csv'):\n",
    "        vectorizer = CountVectorizer(analyzer='word', max_features=1500, tokenizer=weka_tokenizer) \n",
    "    else:\n",
    "        vectorizer = CountVectorizer(analyzer='word', max_features=1500, tokenizer=weka_tokenizer) \n",
    "\n",
    "    bowToken = vectorizer.fit_transform(df['token'])\n",
    "\n",
    "    bowData = pd.DataFrame(bowToken.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    df.drop('token', axis=1, inplace=True)\n",
    "    df = df.join(bowData)\n",
    "    df.drop('klass', axis=1, inplace=True)\n",
    "    if (fileExtension[1] == 'csv'):\n",
    "        df.drop('project', axis=1, inplace=True)\n",
    "    x = df\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victorpires/Library/Python/3.9/lib/python/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# x, y = initDataset('../datasets/flakies_rq_21.csv', '')\n",
    "# x, y = initDataset('../datasets/flakies_rq_22.csv', '')\n",
    "x, y = initDataset('../datasets/MSR4FlakinessOriginal.arff', '')\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "for train_index, test_index in skf.split(x,y):\n",
    "    x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
